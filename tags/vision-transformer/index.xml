<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vision transformer on The AI Talks</title>
    <link>http://theaitalks.org/tags/vision-transformer/</link>
    <description>Recent content in vision transformer on The AI Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://theaitalks.org/tags/vision-transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Architectures and Training for Visual Understanding</title>
      <link>http://theaitalks.org/talks/2022/1006/</link>
      <pubDate>Thu, 06 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1006/</guid>
      <description>Speaker Bio: Hugo Touvron is a research scientist at Meta AI Research. During his PhD he was advised by Hervé Jégou and Matthieu Cord. His current research interests include image classification, transfer learning &amp;amp; fine-grained recognition, with an emphasis on the interplay between architectures and training procedures.
Homepage: https://scholar.google.com/citations?user=xImarzoAAAAJ&amp;amp;hl=en
Abstract Deep learning success is often associated with emblematic architectures. Almost everyone has heard of AlexNet, ResNet or GPT. These successes were also powered by well designed optimisation procedures, which are not usually central to the discussion.</description>
    </item>
    
    <item>
      <title>Large-Scale Visual Representation Learning with Vision Transformers</title>
      <link>http://theaitalks.org/talks/2022/0922/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0922/</guid>
      <description>Speaker Bio: Xiaohua Zhai is a staff researcher and a manager in the Google Research, Brain team, Zürich. He received his PhD degree from Peking University in 2014. His research interests include large-scale representation learning, multimodal learning, transfer learning and self-supervised learning.
Homepage: https://sites.google.com/view/xzhai.
Abstract Attention-based neural networks such as Vision Transformers (ViT) [1] have recently achieved state-of-the-art results on many computer vision benchmarks (e.g. the Visual Task Adaptation Benchmark [2]).</description>
    </item>
    
  </channel>
</rss>
