<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>continual learning on The AI Talks</title>
    <link>http://theaitalks.org/tags/continual-learning/</link>
    <description>Recent content in continual learning on The AI Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 17 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="http://theaitalks.org/tags/continual-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prompting-based Continual Learning</title>
      <link>http://theaitalks.org/talks/2022/1117/</link>
      <pubDate>Thu, 17 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/1117/</guid>
      <description>Speaker Bio: Zifeng Wang is a Ph.D. student at Northeastern University. He received his B.S. degree in Electronic Engineering from Tsinghua University. His research interests include continual (lifelong) learning, data-efficient and parameter-efficient learning, adversarial robustness, and real-world machine learning applications.
Homepage: https://kingspencer.github.io/.
Abstract The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. In this talk, we present a new continual learning paradigm â€“ Prompting-based Continual Learning, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially.</description>
    </item>
    
  </channel>
</rss>
