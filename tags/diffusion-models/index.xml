<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>diffusion models on The AI Talks</title>
    <link>http://theaitalks.org/tags/diffusion-models/</link>
    <description>Recent content in diffusion models on The AI Talks</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 14 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://theaitalks.org/tags/diffusion-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Collecting and Leveraging Data without Crowd Workers</title>
      <link>http://theaitalks.org/talks/2023/0914/</link>
      <pubDate>Thu, 14 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0914/</guid>
      <description>Speaker Yuval Kirstain: During my PhD in generative AI under Professor Omer Levy, I specialized in natural language processing and text-to-image generation. My research has been devoted to developing solutions for acquiring and utilizing task-specific data without the need for traditional crowd workers, employing self-supervised training techniques and gamification instead. My practical experience includes two internships at Facebook AI Research (FAIR), focusing on text-to-image generation and editing. Prior to FAIR, I worked on end-to-end task-oriented dialogue during an internship at IBM Research and gained comprehensive experience in training and evaluating large language models while working at AI21 Labs.</description>
    </item>
    
    <item>
      <title>Customizing Large-Scale Generative Models</title>
      <link>http://theaitalks.org/talks/2023/0406/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0406/</guid>
      <description>Speaker Nupur Kumari is a Ph.D. student at Carnegie Mellon University, advised by Jun-Yan Zhu. Her research is in generative models, specifically efficient fine-tuning and transfer learning techniques to improve generative models.
Abstract Advancements in large-scale generative models represent a watershed moment. These models can generate a wide variety of objects, styles, and scenes and their compositions. However, as end users, we often wish to synthesize specific concepts from our own personal lives.</description>
    </item>
    
    <item>
      <title>Personalizing Text-to-image Generation</title>
      <link>http://theaitalks.org/talks/2023/0209/</link>
      <pubDate>Thu, 09 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2023/0209/</guid>
      <description>Speaker Rinon Gal is a Ph.D. student at Tel Aviv University where he is supervised by Prof. Daniel Cohen-Or and Dr. Amit Bermano. His research focuses on generative models, few-shot and unsupervised approaches, and on combining vision and language. Recently, Rinon has been interning at NVIDIA Research, where he is working on personalization of vision and language models.
Homepage: https://rinongal.github.io
Abstract Text-to-image models offer unprecedented freedom to guide creation through natural language.</description>
    </item>
    
    <item>
      <title>Bit Diffusion: Generating Discrete Data using Diffusion Models with Analog Bits and Self-Conditioning</title>
      <link>http://theaitalks.org/talks/2022/0929/</link>
      <pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>http://theaitalks.org/talks/2022/0929/</guid>
      <description>Speaker Bio: Ting Chen is a research scientist in the Google Brain team. His current research interests include self-supervised representation learning, generative modeling, efficient architectures and generalist learning principles. Before joining Google, he received his Ph.D. in Computer Science from UCLA.
Homepage: https://scholar.google.com/citations?user=KoXUMbsAAAAJ&amp;amp;hl=en.
Abstract We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits.</description>
    </item>
    
  </channel>
</rss>
